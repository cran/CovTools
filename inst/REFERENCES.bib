
@article{bickel_covariance_2008,
	title = {Covariance regularization by thresholding},
	volume = {36},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1231165180},
	doi = {10.1214/08-AOS600},
	language = {en},
	number = {6},
	urldate = {2017-12-01},
	journal = {The Annals of Statistics},
	author = {Bickel, Peter J. and Levina, Elizaveta},
	month = dec,
	year = {2008},
	pages = {2577--2604},
	file = {[Bickel_Levina.2008] Covariance regularization by thresholding.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Bickel_Levina.2008] Covariance regularization by thresholding.pdf:application/pdf}
}

@article{fan_overview_2016,
	title = {An overview of the estimation of large covariance and precision matrices: {Large} covariance estimation},
	volume = {19},
	issn = {13684221},
	shorttitle = {An overview of the estimation of large covariance and precision matrices},
	url = {http://doi.wiley.com/10.1111/ectj.12061},
	doi = {10.1111/ectj.12061},
	language = {en},
	number = {1},
	urldate = {2017-12-01},
	journal = {The Econometrics Journal},
	author = {Fan, Jianqing and Liao, Yuan and Liu, Han},
	month = feb,
	year = {2016},
	pages = {C1--C32},
	file = {[Fan.2016] An overview of the estimation of large covariance and precision matrices.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/[Fan.2016] An overview of the estimation of large covariance and precision matrices.pdf:application/pdf}
}

@article{arsigny_log-euclidean_2006,
	title = {Log-{Euclidean} metrics for fast and simple calculus on diffusion tensors},
	volume = {56},
	issn = {0740-3194, 1522-2594},
	url = {http://doi.wiley.com/10.1002/mrm.20965},
	doi = {10.1002/mrm.20965},
	language = {en},
	number = {2},
	urldate = {2017-12-03},
	journal = {Magnetic Resonance in Medicine},
	author = {Arsigny, Vincent and Fillard, Pierre and Pennec, Xavier and Ayache, Nicholas},
	month = aug,
	year = {2006},
	pages = {411--421},
	file = {[Arsigny.2006] Log-Euclidean metrics for fast and simple calculus on diffusion tensors.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S3. Metric/[Arsigny.2006] Log-Euclidean metrics for fast and simple calculus on diffusion tensors.pdf:application/pdf}
}

@incollection{weickert_symmetric_2006,
	address = {Berlin, Heidelberg},
	title = {Symmetric {Positive}-{Definite} {Matrices}: {From} {Geometry} to {Applications} and {Visualization}},
	isbn = {978-3-540-25032-6 978-3-540-31272-7},
	shorttitle = {Symmetric {Positive}-{Definite} {Matrices}},
	url = {http://link.springer.com/10.1007/3-540-31272-2_17},
	urldate = {2017-12-03},
	booktitle = {Visualization and {Processing} of {Tensor} {Fields}},
	publisher = {Springer Berlin Heidelberg},
	author = {Moakher, Maher and Batchelor, Philipp G.},
	editor = {Weickert, Joachim and Hagen, Hans},
	year = {2006},
	note = {DOI: 10.1007/3-540-31272-2\_17},
	pages = {285--298},
	file = {[Moakher_Batchelor.2006] Symmetric Positive-Definite Matrices.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S3. Metric/[Moakher_Batchelor.2006] Symmetric Positive-Definite Matrices.pdf:application/pdf}
}

@inproceedings{cherian_efficient_2011,
	title = {Efficient similarity search for covariance matrices via the {Jensen}-{Bregman} {LogDet} {Divergence}},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126523/},
	doi = {10.1109/ICCV.2011.6126523},
	urldate = {2017-12-03},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Cherian, Anoop and Sra, Suvrit and Banerjee, Arindam and Papanikolopoulos, Nikolaos},
	month = nov,
	year = {2011},
	pages = {2399--2406},
	file = {[Cherian.2011] Efficient similarity search for covariance matrices via the Jensen-Bregman.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S3. Metric/[Cherian.2011] Efficient similarity search for covariance matrices via the Jensen-Bregman.pdf:application/pdf}
}

@article{engel_overview_2017,
	title = {An overview of large-dimensional covariance and precision matrix estimators with applications in chemometrics},
	volume = {31},
	issn = {08869383},
	url = {http://doi.wiley.com/10.1002/cem.2880},
	doi = {10.1002/cem.2880},
	language = {en},
	number = {4},
	urldate = {2017-12-03},
	journal = {Journal of Chemometrics},
	author = {Engel, Jasper and Buydens, Lutgarde and Blanchet, Lionel},
	month = apr,
	year = {2017},
	pages = {e2880},
	file = {[Engel.2017] An overview of large-dimensional covariance and precision matrix estimators.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/[Engel.2017] An overview of large-dimensional covariance and precision matrix estimators.pdf:application/pdf}
}

@article{cai_estimating_2016,
	title = {Estimating structured high-dimensional covariance and precision matrices: {Optimal} rates and adaptive estimation},
	volume = {10},
	issn = {1935-7524},
	shorttitle = {Estimating structured high-dimensional covariance and precision matrices},
	url = {http://projecteuclid.org/euclid.ejs/1455715952},
	doi = {10.1214/15-EJS1081},
	language = {en},
	number = {1},
	urldate = {2017-12-03},
	journal = {Electronic Journal of Statistics},
	author = {Cai, T. Tony and Ren, Zhao and Zhou, Harrison H.},
	year = {2016},
	pages = {1--59},
	file = {[Cai.2016] Estimating structured high-dimensional covariance and precision matrices.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/[Cai.2016] Estimating structured high-dimensional covariance and precision matrices.pdf:application/pdf}
}

@article{dryden_non-euclidean_2009,
	title = {Non-{Euclidean} statistics for covariance matrices, with applications to diffusion tensor imaging},
	volume = {3},
	issn = {1932-6157},
	url = {http://projecteuclid.org/euclid.aoas/1254773280},
	doi = {10.1214/09-AOAS249},
	language = {en},
	number = {3},
	urldate = {2017-12-03},
	journal = {The Annals of Applied Statistics},
	author = {Dryden, Ian L. and Koloydenko, Alexey and Zhou, Diwei},
	month = sep,
	year = {2009},
	pages = {1102--1123},
	file = {[Dryden.2009] Non-Euclidean statistics for covariance matrices, with applications to.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S4. Mean/[Dryden.2009] Non-Euclidean statistics for covariance matrices, with applications to.pdf:application/pdf}
}

@article{alvarez_bayesian_2014,
	title = {Bayesian inference for a covariance matrix},
	journal = {ArXiv e-prints},
	author = {Alvarez, I. and Niemi, J. and Simpson, M.},
	year = {2014},
	keywords = {Statistics - Methodology},
	file = {[Alvarez.2014] Bayesian inference for a covariance matrix.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S6. EstBayes/[Alvarez.2014] Bayesian inference for a covariance matrix.pdf:application/pdf}
}

@article{barnard_modeling_2000,
	title = {Modeling covariance matrices in terms of standard deviations and correlations, with application to shrinkage},
	volume = {10},
	issn = {1017-0405},
	abstract = {The covariance matrix plays an important role in statistical inference, yet modeling a covariance matrix is often a difficult task in practice due to its dimensionality and the non-negative definite constraint. In order to model a covariance matrix effectively, it is typically broken down into components based on modeling considerations or mathematical convenience. Decompositions that have received recent research attention include variance components, spectral decomposition, Cholesky decomposition, and matrix logarithm. In this paper we study a statistically motivated decomposition which appears to be relatively unexplored for the purpose of modeling. We model a covariance matrix in terms of its corresponding standard deviations and correlation matrix. We discuss two general modeling situations where this approach is useful: shrinkage estimation of regression coefficients, and a general location-scale model for both categorical and continuous variables. We present some simple choices for priors in terms of standard deviations and the correlation matrix, and describe a straightforward computational strategy for obtaining the posterior of the covariance matrix. We apply our method to real and simulated data sets in the context of shrinkage estimation.},
	number = {4},
	journal = {Statistica Sinica},
	author = {Barnard, John and McCulloch, Robert and Meng, Xiao Li},
	year = {2000},
	keywords = {General location model, General location-scale model, Gibbs sampler, Hierarchical models, Markov chain Monte Carlo, Wishart distribution},
	pages = {1281--1311},
	file = {[Barnard.2000] Modeling covariance matrices in terms of standard deviations and correlations,.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S6. EstBayes/[Barnard.2000] Modeling covariance matrices in terms of standard deviations and correlations,.pdf:application/pdf}
}

@article{cai_adaptive_2011,
	title = {Adaptive {Thresholding} for {Sparse} {Covariance} {Matrix} {Estimation}},
	volume = {106},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10560},
	doi = {10.1198/jasa.2011.tm10560},
	language = {en},
	number = {494},
	urldate = {2017-12-12},
	journal = {Journal of the American Statistical Association},
	author = {Cai, Tony and Liu, Weidong},
	month = jun,
	year = {2011},
	pages = {672--684},
	file = {[Cai_Liu.2011] Adaptive Thresholding for Sparse Covariance Matrix Estimation.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Cai_Liu.2011] Adaptive Thresholding for Sparse Covariance Matrix Estimation.pdf:application/pdf}
}

@incollection{grafarend_metric_2003,
	address = {Berlin, Heidelberg},
	title = {A {Metric} for {Covariance} {Matrices}},
	isbn = {978-3-642-07733-3 978-3-662-05296-9},
	url = {http://link.springer.com/10.1007/978-3-662-05296-9_31},
	language = {en},
	urldate = {2017-12-12},
	booktitle = {Geodesy-{The} {Challenge} of the 3rd {Millennium}},
	publisher = {Springer Berlin Heidelberg},
	author = {FÃ¶rstner, Wolfgang and Moonen, Boudewijn},
	editor = {Grafarend, Erik W. and Krumm, Friedrich W. and Schwarze, Volker S.},
	year = {2003},
	note = {DOI: 10.1007/978-3-662-05296-9\_31},
	pages = {299--309},
	file = {[FÃ¶rstner_Moonen.2003] A Metric for Covariance Matrices.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S3. Metric/[FÃ¶rstner_Moonen.2003] A Metric for Covariance Matrices.pdf:application/pdf}
}

@article{donoho_wavelet_1995,
	title = {Wavelet {Shrinkage}: {Asymptopia}?},
	volume = {57},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2345967},
	abstract = {Much recent effort has sought asymptotically minimax methods for recovering infinite dimensional objectsâcurves, densities, spectral densities, imagesâfrom noisy data. A now rich and complex body of work develops nearly or exactly minimax estimators for an array of interesting problems. Unfortunately, the results have rarely moved into practice, for a variety of reasonsâamong them being similarity to known methods, computational intractability and lack of spatial adaptivity. We discuss a arise from iterated mappings of the centre of the interval to which the map is applied. Particular attention is paid to the shape of an invariant distribution in the tails or in the neighbourhood of a pole of its density. A new technique is developed for this application. It enables us to combine `parametric' information, available from the structure of the map, with `nonparametric' information obtainable from numerical experiments.},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Donoho, David L. and Johnstone, Iain M. and Kerkyacharian, Gerard and Picard, Dominique},
	year = {1995},
	pages = {301--369},
	file = {[Donoho.1995] Wavelet Shrinkage.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Donoho.1995] Wavelet Shrinkage.pdf:application/pdf}
}

@article{antoniadis_regularization_2001,
	title = {Regularization of {Wavelet} {Approximations}},
	volume = {96},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753208942},
	doi = {10.1198/016214501753208942},
	language = {en},
	number = {455},
	urldate = {2018-02-11},
	journal = {Journal of the American Statistical Association},
	author = {Antoniadis, Anestis and Fan, Jianqing},
	month = sep,
	year = {2001},
	pages = {939--967},
	file = {[Antoniadis_Fan.2001] Regularization of Wavelet Approximations.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Antoniadis_Fan.2001] Regularization of Wavelet Approximations.pdf:application/pdf}
}

@article{fan_variable_2001,
	title = {Variable {Selection} via {Nonconcave} {Penalized} {Likelihood} and {Its} {Oracle} {Properties}},
	volume = {96},
	issn = {01621459},
	url = {http://www.jstor.org/stable/3085904},
	abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, â), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
	number = {456},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Li, Runze},
	year = {2001},
	pages = {1348--1360},
	file = {[Fan_Li.2001] Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Fan_Li.2001] Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties.pdf:application/pdf}
}

@article{antoniadis_wavelets_1997,
	title = {Wavelets in statistics: {A} review},
	volume = {6},
	issn = {1613-981X},
	url = {https://doi.org/10.1007/BF03178905},
	doi = {10.1007/BF03178905},
	abstract = {The field of nonparametric function estimation has broadened its appeal in recent years with an array of new tools for statistical analysis. In particular, theoretical and applied research on the field of wavelets has had noticeable influence on statistical topics such as nonparametric regression, nonparametric density estimation, nonparametric discrimination and many other related topics. This is a survey article that attempts to synthetize a broad variety of work on wavelets in statistics and includes some recent developments in nonparametric curve estimation that have been omitted from review articles and books on the subject. After a short introduction to wavelet theory, wavelets are treated in the familiar context of estimation of Â«smoothÂ» functions. Both Â«linearÂ» and Â«nonlinearÂ» wavelet estimation methods are discussed and cross-validation methods for choosing the smoothing parameters are addressed. Finally, some areas of related research are mentioned, such as hypothesis testing, model selection, hazard rate estimation for censored data, and nonparametric change-point problems. The closing section formulates some promising research directions relating to wavelets in statistics.},
	number = {2},
	journal = {Journal of the Italian Statistical Society},
	author = {Antoniadis, A.},
	month = aug,
	year = {1997},
	pages = {97},
	file = {[Antoniadis.1997] Wavelets in statistics.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Antoniadis.1997] Wavelets in statistics.pdf:application/pdf}
}

@article{zhang_nearly_2010,
	title = {Nearly unbiased variable selection under minimax concave penalty},
	volume = {38},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1266586618},
	doi = {10.1214/09-AOS729},
	language = {en},
	number = {2},
	urldate = {2018-02-11},
	journal = {The Annals of Statistics},
	author = {Zhang, Cun-Hui},
	month = apr,
	year = {2010},
	pages = {894--942},
	file = {[Zhang.2010] Nearly unbiased variable selection under minimax concave penalty.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Zhang.2010] Nearly unbiased variable selection under minimax concave penalty.pdf:application/pdf}
}

@article{fan_large_2013,
	title = {Large covariance estimation by thresholding principal orthogonal complements},
	volume = {75},
	issn = {13697412},
	url = {http://doi.wiley.com/10.1111/rssb.12016},
	doi = {10.1111/rssb.12016},
	language = {en},
	number = {4},
	urldate = {2018-02-12},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Fan, Jianqing and Liao, Yuan and Mincheva, Martina},
	month = sep,
	year = {2013},
	pages = {603--680},
	file = {[Fan.2013] Large covariance estimation by thresholding principal orthogonal complements.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Fan.2013] Large covariance estimation by thresholding principal orthogonal complements.pdf:application/pdf}
}

@article{qi_quadratically_2006,
	title = {A {Quadratically} {Convergent} {Newton} {Method} for {Computing} the {Nearest} {Correlation} {Matrix}},
	volume = {28},
	issn = {0895-4798, 1095-7162},
	url = {http://epubs.siam.org/doi/10.1137/050624509},
	doi = {10.1137/050624509},
	language = {en},
	number = {2},
	urldate = {2018-02-12},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Qi, Houduo and Sun, Defeng},
	month = jan,
	year = {2006},
	pages = {360--385},
	file = {[Qi_Sun.2006] A Quadratically Convergent Newton Method for Computing the Nearest Correlation.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Qi_Sun.2006] A Quadratically Convergent Newton Method for Computing the Nearest Correlation.pdf:application/pdf}
}

@article{yuan_model_2007,
	title = {Model {Selection} and {Estimation} in the {Gaussian} {Graphical} {Model}},
	volume = {94},
	issn = {00063444},
	url = {http://www.jstor.org/stable/20441351},
	abstract = {We propose penalized likelihood methods for estimating the concentration matrix in the Gaussian graphical model. The methods lead to a sparse and shrinkage estimator of the concentration matrix that is positive definite, and thus conduct model selection and estimation simultaneously. The implementation of the methods is nontrivial because of the positive definite constraint on the concentration matrix, but we show that the computation can be done effectively by taking advantage of the efficient maxdet algorithm developed in convex optimization. We propose a BIC-type criterion for the selection of the tuning parameter in the penalized likelihood methods. The connection between our methods and existing methods is illustrated. Simulations and real examples demonstrate the competitive performance of the new methods.},
	number = {1},
	journal = {Biometrika},
	author = {Yuan, Ming and Lin, Yi},
	year = {2007},
	pages = {19--35},
	file = {[Yuan_Lin.2007] Model Selection and Estimation in the Gaussian Graphical Model.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S2. PreEst/[Yuan_Lin.2007] Model Selection and Estimation in the Gaussian Graphical Model.pdf:application/pdf}
}

@article{friedman_sparse_2008,
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxm045},
	doi = {10.1093/biostatistics/kxm045},
	language = {en},
	number = {3},
	urldate = {2018-02-12},
	journal = {Biostatistics},
	author = {Friedman, J. and Hastie, T. and Tibshirani, R.},
	month = jul,
	year = {2008},
	pages = {432--441},
	file = {[Friedman.2008] Sparse inverse covariance estimation with the graphical lasso.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S2. PreEst/[Friedman.2008] Sparse inverse covariance estimation with the graphical lasso.pdf:application/pdf}
}

@inproceedings{banerjee_convex_2006,
	title = {Convex optimization techniques for fitting sparse {Gaussian} graphical models},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143856},
	doi = {10.1145/1143844.1143856},
	language = {en},
	urldate = {2018-02-12},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {ACM Press},
	author = {Banerjee, Onureena and Ghaoui, Laurent El and d'Aspremont, Alexandre and Natsoulis, Georges},
	year = {2006},
	pages = {89--96},
	file = {[Banerjee.2006] Convex optimization techniques for fitting sparse Gaussian graphical models.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S2. PreEst/[Banerjee.2006] Convex optimization techniques for fitting sparse Gaussian graphical models.pdf:application/pdf}
}

@article{banerjee_posterior_2014,
	title = {Posterior convergence rates for estimating large precision matrices using graphical models},
	volume = {8},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1414588188},
	doi = {10.1214/14-EJS945},
	language = {en},
	number = {2},
	urldate = {2018-02-13},
	journal = {Electronic Journal of Statistics},
	author = {Banerjee, Sayantan and Ghosal, Subhashis},
	year = {2014},
	pages = {2111--2137},
	file = {[Banerjee_Ghosal.2014] Posterior convergence rates for estimating large precision matrices using.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S2. PreEst/[Banerjee_Ghosal.2014] Posterior convergence rates for estimating large precision matrices using.pdf:application/pdf}
}

@article{lee_estimating_2017,
	title = {Estimating {Large} {Precision} {Matrices} via {Modified} {Cholesky} {Decomposition}},
	journal = {ArXiv e-prints},
	author = {Lee, K. and Lee, J.},
	month = jul,
	year = {2017},
	keywords = {Mathematics - Statistics Theory},
	file = {[Lee_Lee.2017] Estimating Large Precision Matrices via Modified Cholesky Decomposition.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S2. PreEst/[Lee_Lee.2017] Estimating Large Precision Matrices via Modified Cholesky Decomposition.pdf:application/pdf}
}

@article{an_hypothesis_2014,
	title = {Hypothesis testing for band size detection of high-dimensional banded precision matrices},
	volume = {101},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asu006},
	doi = {10.1093/biomet/asu006},
	language = {en},
	number = {2},
	urldate = {2018-02-18},
	journal = {Biometrika},
	author = {An, B. and Guo, J. and Liu, Y.},
	month = jun,
	year = {2014},
	pages = {477--483},
	file = {[An.2014] Hypothesis testing for band size detection of high-dimensional banded precision.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S2. PreEst/[An.2014] Hypothesis testing for band size detection of high-dimensional banded precision.pdf:application/pdf}
}

@article{bickel_regularized_2008,
	title = {Regularized estimation of large covariance matrices},
	volume = {36},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1201877299},
	doi = {10.1214/009053607000000758},
	language = {en},
	number = {1},
	urldate = {2018-02-18},
	journal = {The Annals of Statistics},
	author = {Bickel, Peter J. and Levina, Elizaveta},
	month = feb,
	year = {2008},
	pages = {199--227},
	file = {[Bickel_Levina.2008] Regularized estimation of large covariance matrices.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S2. PreEst/[Bickel_Levina.2008] Regularized estimation of large covariance matrices.pdf:application/pdf}
}

@article{cai_optimal_2013,
	title = {Optimal hypothesis testing for high dimensional covariance matrices},
	volume = {19},
	issn = {1350-7265},
	url = {http://projecteuclid.org/euclid.bj/1386078606},
	doi = {10.3150/12-BEJ455},
	language = {en},
	number = {5B},
	urldate = {2018-02-19},
	journal = {Bernoulli},
	author = {Cai, T. Tony and Ma, Zongming},
	month = nov,
	year = {2013},
	pages = {2359--2388},
	file = {[Cai_Ma.2013] Optimal hypothesis testing for high dimensional covariance matrices.pdf:/home/kisung/Dropbox/V3. Statistics/P2. Covariance/S1. CovEst/[Cai_Ma.2013] Optimal hypothesis testing for high dimensional covariance matrices.pdf:application/pdf}
}